{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b891f7-0d9e-46c4-a0f2-2501e3cab940",
   "metadata": {},
   "source": [
    "# Automated analysis of geochemical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f0ef0-af79-46b4-861b-366da4dd6ede",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d6aca-f4b5-4ef6-928f-decf62f198d5",
   "metadata": {},
   "source": [
    "### Block 1 : Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2e5e3-3db1-409b-8887-8ad89f61684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and basic configuration ---\n",
    "from pathlib import Path\n",
    "import os, math, warnings, json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# For better display in Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"C:/Folder\") # add your folder here\n",
    "INPUT_CSV = DATA_DIR / \"Data.csv\" # add your data source here\n",
    "\n",
    "OUT_DIR_PNG = Path(\"./PNG\"); OUT_DIR_PNG.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR_SHP = Path(\"./SHP\"); OUT_DIR_SHP.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR_CSV = Path(\"./CSV\"); OUT_DIR_CSV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports and directories are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9a37f-63e6-4b71-a135-345536f77132",
   "metadata": {},
   "source": [
    "### Block 2: Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8b365-f9c3-418a-bda5-30843dbdb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CSV and rename coordinate columns ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Rename coordinate columns for consistency\n",
    "rename_map = {\"EAST\":\"x\", \"NORTH\":\"y\", \"RL\":\"z\"} # rename coordinate columns if applicable\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Identify geochemical element columns by excluding metadata\n",
    "exclude_cols = ['PROJECT', 'SAMPLEID', 'x', 'y', 'z'] # exclude useless columns if applicable\n",
    "elements = [c for c in df.columns if c not in exclude_cols]\n",
    "unique_elements_count = len(set(elements))\n",
    "\n",
    "# Basic info\n",
    "print(f\"‚úÖ Rows loaded: {len(df):,}\")\n",
    "print(f\"‚ÑπÔ∏è Unique SAMPLEIDs: {df['SAMPLEID'].nunique():,}\" if 'SAMPLEID' in df.columns else f\"‚ÑπÔ∏è Total samples: {len(df):,}\")\n",
    "print(\"‚ÑπÔ∏è Unique geochemical elements:\", unique_elements_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a2923-9c9d-446e-afad-247874146cd6",
   "metadata": {},
   "source": [
    "### Block 3: Missing Values and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac1dc9-2fcf-4f11-9701-209e4aa885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Missing values in element columns ---\n",
    "missing = df[elements].isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing):\n",
    "    print(\"üîç Missing values in element columns (top 15):\")\n",
    "    display(missing.head(15))\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected in element columns.\")\n",
    "\n",
    "# --- 3.2 Outlier flags: z-score and IQR ---\n",
    "z_thresh = 3\n",
    "z_input = df[elements].copy().fillna(df[elements].median(numeric_only=True))\n",
    "z_mask = np.abs(stats.zscore(z_input, nan_policy='omit')) > z_thresh\n",
    "df['flag_z_outlier'] = pd.DataFrame(z_mask, index=z_input.index, columns=z_input.columns).any(axis=1)\n",
    "\n",
    "Q1 = df[elements].quantile(0.25)\n",
    "Q3 = df[elements].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "iqr_mask = ((df[elements] < lower) | (df[elements] > upper)).fillna(False)\n",
    "df['flag_iqr_outlier'] = iqr_mask.any(axis=1)\n",
    "\n",
    "print(f\"üö© Rows with z-score outliers: {int(df['flag_z_outlier'].sum())}\")\n",
    "print(f\"üö© Rows with IQR outliers: {int(df['flag_iqr_outlier'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60788b8e-677d-4d22-af93-44da81a46fb0",
   "metadata": {},
   "source": [
    "### Block 4: Standardizing Units to ppm (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4fb93-50d9-4965-a0e0-6572d4f1b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: extract unit suffix from column name ---\n",
    "def parse_unit(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts unit suffix from column name: 'ppm', 'ppb', 'pct', or ''.\n",
    "    Examples:\n",
    "        'Cu_ppm' ‚Üí 'ppm'\n",
    "        'Au_ppb' ‚Üí 'ppb'\n",
    "        'Al_pct' ‚Üí 'pct'\n",
    "    \"\"\"\n",
    "    return col.rsplit('_', 1)[-1] if '_' in col else ''\n",
    "\n",
    "# --- Convert selected columns to ppm ---\n",
    "def to_ppm_frame(df_in: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts specified columns to ppm:\n",
    "    - ppb ‚Üí /1000\n",
    "    - pct ‚Üí *1e4\n",
    "    - ppm or unknown ‚Üí unchanged\n",
    "    Prints list of converted columns.\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df_in.index)\n",
    "    converted = []\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(df_in[c], errors='coerce')\n",
    "        u = parse_unit(c)\n",
    "        if u == 'ppb':\n",
    "            out[c] = s / 1000.0\n",
    "            converted.append(f\"{c} (ppb ‚Üí ppm)\")\n",
    "        elif u == 'pct':\n",
    "            out[c] = s * 1e4\n",
    "            converted.append(f\"{c} (% ‚Üí ppm)\")\n",
    "        else:\n",
    "            out[c] = s\n",
    "    if converted:\n",
    "        print(\"‚úÖ Converted to ppm:\", \", \".join(converted))\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è All elements already in ppm or unitless (no conversion needed).\")\n",
    "    return out\n",
    "\n",
    "# --- Rename columns to reflect ppm conversion ---\n",
    "def rename_to_ppm_columns(df_ppm: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Renames columns with original units (ppb, pct) to *_ppm.\n",
    "    Example:\n",
    "        'Au_ppb' ‚Üí 'Au_ppm'\n",
    "        'Al_pct' ‚Üí 'Al_ppm'\n",
    "    If target name already exists, adds suffix '_from_<unit>'.\n",
    "    Returns (renamed_df, name_map).\n",
    "    \"\"\"\n",
    "    name_map = {}\n",
    "    for c in df_ppm.columns:\n",
    "        unit = parse_unit(c)\n",
    "        if unit in ('ppb', 'pct'):\n",
    "            base = c.rsplit('_', 1)[0]\n",
    "            new_name = f\"{base}_ppm\"\n",
    "            if new_name in df_ppm.columns and new_name != c:\n",
    "                new_name = f\"{base}_ppm_from_{unit}\"\n",
    "            name_map[c] = new_name\n",
    "    df_renamed = df_ppm.rename(columns=name_map)\n",
    "    return df_renamed, name_map\n",
    "\n",
    "# --- Apply conversion and renaming ---\n",
    "df_elems_ppm = to_ppm_frame(df, elements)\n",
    "df_elems_ppm_named, ppm_name_map = rename_to_ppm_columns(df_elems_ppm)\n",
    "\n",
    "if ppm_name_map:\n",
    "    print(\"üî• Renamed columns for clarity (analytics):\")\n",
    "    for old, new in ppm_name_map.items():\n",
    "        print(f\" {old} ‚Üí {new}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No renaming needed: all element columns already in *_ppm format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd09a6b-8c44-4d68-9f16-64d5561ff368",
   "metadata": {},
   "source": [
    "### Block 5: CLR Transformation and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0905de7-d04c-4c58-bfc7-ab44fe95b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLR transformation ---\n",
    "def clr_transform(df_sub: pd.DataFrame, pseudocount: float = 1e-6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs classical CLR (Centered Log-Ratio) transformation:\n",
    "    ln(x + pseudocount) - mean(ln(x + pseudocount)) across each row.\n",
    "    Returns a DataFrame with columns named '<element>_CLR'.\n",
    "    \"\"\"\n",
    "    arr = df_sub.fillna(0).values.astype(float) + pseudocount\n",
    "    log_arr = np.log(arr)\n",
    "    gm = log_arr.mean(axis=1)\n",
    "    clr_arr = log_arr - gm[:, None]\n",
    "    return pd.DataFrame(clr_arr, index=df_sub.index,\n",
    "                        columns=[f\"{c}_CLR\" for c in df_sub.columns])\n",
    "\n",
    "# --- Apply CLR transformation to renamed ppm element set ---\n",
    "df_clr_ppm_named = clr_transform(df_elems_ppm_named)\n",
    "\n",
    "# --- Calculate mean (mu) and standard deviation (sigma) for CLR-transformed columns ---\n",
    "stats_df = pd.DataFrame(index=df_clr_ppm_named.columns)\n",
    "stats_df['mu'] = df_clr_ppm_named.mean()\n",
    "stats_df['sigma'] = df_clr_ppm_named.std(ddof=1)\n",
    "\n",
    "print(\"‚úÖ CLR transformation completed.\")\n",
    "# display(stats_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8376d-3857-46da-a376-d98a3033ac79",
   "metadata": {},
   "source": [
    "### Block 6: Variability Metrics ‚Äî Robust CV, SD(log), Aitchison Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095dbcb-722a-4fec-a38f-23492bb9d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust CV (RAW, %) using MAD/median ---\n",
    "def robust_cv_raw(series: pd.Series, scale_mad: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Calculates robust coefficient of variation (CV) in percent:\n",
    "    CV (%) = 100 * (k * MAD / median)\n",
    "    Suitable for raw ppm values.\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors='coerce').dropna().to_numpy()\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    med = float(np.median(x))\n",
    "    if med == 0:\n",
    "        return np.nan\n",
    "    mad = float(np.median(np.abs(x - med)))\n",
    "    k = 1.4826 if scale_mad else 1.0\n",
    "    return 100.0 * (k * mad / med)\n",
    "\n",
    "# --- SD(log x) for positive ppm values ---\n",
    "def sd_log(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Computes standard deviation of log-transformed values.\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors='coerce')\n",
    "    x = x[x > 0]\n",
    "    if x.size < 2:\n",
    "        return np.nan\n",
    "    y = np.log(x.to_numpy(dtype=float))\n",
    "    return float(np.std(y, ddof=1))\n",
    "\n",
    "# --- Aitchison variance for CLR-transformed components ---\n",
    "def aitchison_var_component(clr_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Computes Aitchison variance of CLR-transformed component.\n",
    "    \"\"\"\n",
    "    z = pd.to_numeric(clr_series, errors='coerce').dropna().to_numpy()\n",
    "    if z.size < 2:\n",
    "        return np.nan\n",
    "    return float(np.var(z, ddof=1))\n",
    "\n",
    "# --- Compute metrics ---\n",
    "cv_raw_df = pd.DataFrame({\n",
    "    'CV_%': {col: robust_cv_raw(df_elems_ppm_named[col], scale_mad=True)\n",
    "             for col in df_elems_ppm_named.columns}\n",
    "})\n",
    "\n",
    "sd_log_df = pd.DataFrame({\n",
    "    'SD_log': {col: sd_log(df_elems_ppm_named[col])\n",
    "               for col in df_elems_ppm_named.columns}\n",
    "})\n",
    "\n",
    "aitch_var_df = pd.DataFrame({\n",
    "    'AitchisonVar': {\n",
    "        col.replace('_CLR', ''): aitchison_var_component(df_clr_ppm_named[col])\n",
    "        for col in df_clr_ppm_named.columns\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"üèÅ Top 20 by Robust CV (RAW, %):\")\n",
    "display(cv_raw_df.sort_values('CV_%', ascending=False).head(20))\n",
    "\n",
    "print(\"üèÅ Top 20 by SD(log x):\")\n",
    "display(sd_log_df.sort_values('SD_log', ascending=False).head(20))\n",
    "\n",
    "print(\"üèÅ Top 20 by Aitchison variance (CLR):\")\n",
    "display(aitch_var_df.sort_values('AitchisonVar', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcba9d-af5b-48f1-934c-e60fb0ca51b2",
   "metadata": {},
   "source": [
    "### Block 7: Exporting Metrics and Element Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afdc1b-e0c0-4fb7-9f6b-4b9be382210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: export metric to CSV ---\n",
    "def _export_metric(df_onecol: pd.DataFrame, col_name: str, fname: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exports a single-column metric DataFrame to CSV with two columns: Element, <metric>.\n",
    "    Returns the sorted DataFrame (descending).\n",
    "    \"\"\"\n",
    "    out = (\n",
    "        df_onecol.rename_axis('Element')\n",
    "        .reset_index()\n",
    "        .sort_values(col_name, ascending=False)\n",
    "    )\n",
    "    out.to_csv(OUT_DIR_CSV / fname, index=False, encoding=\"utf-8-sig\")\n",
    "    return out\n",
    "\n",
    "# --- Helper: get top-k elements by metric ---\n",
    "def _topk(df_onecol: pd.DataFrame, col_name: str, k: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns top-k rows from DataFrame by column col_name (descending).\n",
    "    \"\"\"\n",
    "    return df_onecol.nlargest(min(k, len(df_onecol)), columns=col_name)\n",
    "\n",
    "# --- Export each metric to CSV ---\n",
    "cv_raw_out = _export_metric(cv_raw_df, 'CV_%', \"cv_raw_ppm_robust_mad_median.csv\")\n",
    "sd_log_out = _export_metric(sd_log_df, 'SD_log', \"sd_log_ppm.csv\")\n",
    "aitch_out = _export_metric(aitch_var_df, 'AitchisonVar', \"aitchison_variance_clr.csv\")\n",
    "\n",
    "print(\"üìÅ Metrics exported to CSV:\")\n",
    "print(f\" ‚Ä¢ Robust CV (RAW, %) ‚Üí {OUT_DIR_CSV / 'cv_raw_ppm_robust_mad_median.csv'}\")\n",
    "print(f\" ‚Ä¢ SD(log x) (LOG) ‚Üí {OUT_DIR_CSV / 'sd_log_ppm.csv'}\")\n",
    "print(f\" ‚Ä¢ AitchisonVar (CLR) ‚Üí {OUT_DIR_CSV / 'aitchison_variance_clr.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01dea2-75ac-4e61-b394-c7a92a1db5c7",
   "metadata": {},
   "source": [
    "### Block 8: Integrated Element Ranking by Mean Metric Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2047d2d-6c3d-4b6b-b438-89c80f5b0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine metrics into a single DataFrame ---\n",
    "cv_comparison = (\n",
    "    cv_raw_out[['Element', 'CV_%']]\n",
    "    .merge(sd_log_out[['Element', 'SD_log']], on='Element', how='outer')\n",
    "    .merge(aitch_out[['Element', 'AitchisonVar']], on='Element', how='outer')\n",
    ")\n",
    "\n",
    "# --- Rank elements by each metric (higher value = better rank) ---\n",
    "rank_df = cv_comparison.copy()\n",
    "rank_df['Rank_CV'] = rank_df['CV_%'].rank(ascending=False, method='min')\n",
    "rank_df['Rank_SDlog'] = rank_df['SD_log'].rank(ascending=False, method='min')\n",
    "rank_df['Rank_AitchVar'] = rank_df['AitchisonVar'].rank(ascending=False, method='min')\n",
    "\n",
    "# --- Fill missing ranks with worst possible rank + 1 ---\n",
    "max_rank = max(\n",
    "    rank_df['Rank_CV'].max(skipna=True),\n",
    "    rank_df['Rank_SDlog'].max(skipna=True),\n",
    "    rank_df['Rank_AitchVar'].max(skipna=True)\n",
    ")\n",
    "rank_cols = ['Rank_CV', 'Rank_SDlog', 'Rank_AitchVar']\n",
    "rank_df[rank_cols] = rank_df[rank_cols].fillna(max_rank + 1)\n",
    "\n",
    "# --- Calculate mean rank and select top 20 elements ---\n",
    "rank_df['MeanRank'] = rank_df[rank_cols].mean(axis=1)\n",
    "top20_meanrank = rank_df.nsmallest(20, 'MeanRank').reset_index(drop=True)\n",
    "\n",
    "print(\"üèÜ Top 20 elements by mean rank (integrated variability score):\")\n",
    "display(top20_meanrank[['Element', 'CV_%', 'SD_log', 'AitchisonVar',\n",
    "                        'Rank_CV', 'Rank_SDlog', 'Rank_AitchVar', 'MeanRank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af756e4-886d-44fb-a7ae-56ba9d7a2076",
   "metadata": {},
   "source": [
    "### Block 9: Exporting Summary and Visualizing Top Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636168b-8664-4274-a917-154eb647d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export combined metrics and top-20 by mean rank ---\n",
    "cmp_path = OUT_DIR_CSV / \"metrics_combined_cv_sdlog_aitch.csv\"\n",
    "top_path = OUT_DIR_CSV / \"selected_top20_by_meanrank.csv\"\n",
    "\n",
    "cv_comparison.to_csv(cmp_path, index=False, encoding=\"utf-8-sig\")\n",
    "top20_meanrank[['Element', 'CV_%', 'SD_log', 'AitchisonVar', 'MeanRank']].to_csv(\n",
    "    top_path, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(\"üìÅ Export completed:\")\n",
    "print(f\" ‚Ä¢ Combined metrics ‚Üí {cmp_path}\")\n",
    "print(f\" ‚Ä¢ TOP-20 (MeanRank) ‚Üí {top_path}\")\n",
    "\n",
    "# --- Normalized comparison plot of top-20 elements ---\n",
    "common_idx = top20_meanrank['Element']\n",
    "cmp20 = (\n",
    "    cv_raw_df.join(sd_log_df, how='outer')\n",
    "    .join(aitch_var_df, how='outer')\n",
    ").loc[common_idx]\n",
    "\n",
    "cmp20.columns = ['Robust CV (%)', 'SD(log x)', 'AitchisonVar']\n",
    "norm20 = (cmp20 - cmp20.min()) / (cmp20.max() - cmp20.min())\n",
    "\n",
    "ax = norm20.plot(kind='bar', figsize=(12, 6))\n",
    "ax.set_title('Normalized variability metrics for top-20 elements')\n",
    "ax.set_ylabel('Normalized value [0..1]')\n",
    "ax.set_xlabel('Element')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = OUT_DIR_PNG / 'cmp20_metrics_normalized.png'\n",
    "plt.savefig(out_png, dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"‚úÖ PNG saved: {out_png.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445af2be-1c35-45e7-a0d8-1616406de1f9",
   "metadata": {},
   "source": [
    "### Block 10: Correlation Analysis and Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d0fa9-410f-4267-97be-9517669b9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for correlation analysis ---\n",
    "CORR_METHOD = \"spearman\"\n",
    "CORR_CUTOFF = 0.70  # Threshold for strong correlation\n",
    "\n",
    "# --- Use top-20 elements from previous ranking as base ---\n",
    "pre_selected_elements = top20_meanrank['Element'].tolist()\n",
    "\n",
    "# --- Full correlation matrix (Spearman) ---\n",
    "correlation_matrix = df_elems_ppm_named.corr(method=CORR_METHOD)\n",
    "\n",
    "# --- Find additional elements strongly correlated with base elements ---\n",
    "additional_elements = set()\n",
    "for base_el in pre_selected_elements:\n",
    "    if base_el in correlation_matrix.columns:\n",
    "        col = correlation_matrix[base_el]\n",
    "        mask = (col.abs() >= CORR_CUTOFF) & (col.index != base_el)\n",
    "        correlated = col[mask]\n",
    "        for el in correlated.index:\n",
    "            if el not in pre_selected_elements:\n",
    "                additional_elements.add(el)\n",
    "\n",
    "print(f\"‚ûï Additional elements with |œÅ| ‚â• {CORR_CUTOFF}:\")\n",
    "print(sorted(additional_elements) if additional_elements else \"‚Äî none ‚Äî\")\n",
    "\n",
    "# --- Prepare list of elements for heatmap ---\n",
    "sel_exist = [c for c in pre_selected_elements if c in df_elems_ppm_named.columns]\n",
    "add_exist = [c for c in additional_elements if c in df_elems_ppm_named.columns]\n",
    "all_for_heatmap = sel_exist + [c for c in add_exist if c not in sel_exist]\n",
    "\n",
    "# --- Subset correlation matrix and plot heatmap ---\n",
    "corr_sub = df_elems_ppm_named[all_for_heatmap].corr(method=CORR_METHOD)\n",
    "\n",
    "plt.figure(figsize=(0.6 * len(all_for_heatmap) + 4, 0.6 * len(all_for_heatmap) + 2))\n",
    "sns.heatmap(corr_sub, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            cbar_kws={'shrink': 0.8}, square=True)\n",
    "plt.title(\"Spearman Correlation Matrix for Selected Elements\")\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap_path = OUT_DIR_PNG / 'correlationMatrix_preSelEl.png'\n",
    "plt.savefig(heatmap_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Heatmap saved: {heatmap_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622090a-d464-4bc0-a2fb-29ed65576a5b",
   "metadata": {},
   "source": [
    "### Block 11: Anomaly Thresholds and Element Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9afa6-3a65-4f1f-a1f4-1eb72c67818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary table: anomaly thresholds and counts ---\n",
    "summary_rows = []\n",
    "for el in all_for_heatmap:\n",
    "    s = pd.to_numeric(df_elems_ppm_named[el], errors=\"coerce\").dropna()\n",
    "    s_pos = s[s > 0]\n",
    "    if s_pos.empty:\n",
    "        summary_rows.append({\n",
    "            \"Element\": el,\n",
    "            \"Median (BG)\": np.nan,\n",
    "            \"Geometric Mean (BG)\": np.nan,\n",
    "            \"Anomaly Threshold (Œº+2œÉ)\": np.nan,\n",
    "            \"Anomaly Threshold (95th pct)\": np.nan,\n",
    "            \"Anomaly Count (> Œº+2œÉ)\": 0\n",
    "        })\n",
    "        continue\n",
    "    log_vals = np.log(s_pos)\n",
    "    mu = log_vals.mean()\n",
    "    sigma = log_vals.std(ddof=1)\n",
    "    thr_log = float(np.exp(mu + 2.0 * sigma))\n",
    "    thr_p95 = float(np.percentile(s_pos, 95))\n",
    "    anomalies = s_pos[s_pos > thr_log]\n",
    "    summary_rows.append({\n",
    "        \"Element\": el,\n",
    "        \"Median (BG)\": round(float(np.median(s_pos)), 4),\n",
    "        \"Geometric Mean (BG)\": round(float(np.exp(mu)), 4),\n",
    "        \"Anomaly Threshold (Œº+2œÉ)\": round(thr_log, 4),\n",
    "        \"Anomaly Threshold (95th pct)\": round(thr_p95, 4),\n",
    "        \"Anomaly Count (> Œº+2œÉ)\": int(len(anomalies))\n",
    "    })\n",
    "\n",
    "BG_AN_summary_df = pd.DataFrame(summary_rows)\n",
    "display(BG_AN_summary_df.sort_values(\"Anomaly Count (> Œº+2œÉ)\", ascending=False))\n",
    "\n",
    "# --- Derived metrics ---\n",
    "BG_AN_summary_df[\"k_BG\"] = BG_AN_summary_df[\"Anomaly Threshold (Œº+2œÉ)\"] / BG_AN_summary_df[\"Median (BG)\"]\n",
    "BG_AN_summary_df[\"tail_gap_R\"] = BG_AN_summary_df[\"Anomaly Threshold (Œº+2œÉ)\"] / BG_AN_summary_df[\"Anomaly Threshold (95th pct)\"]\n",
    "\n",
    "# --- Classification rules ---\n",
    "MIN_ANOM_COUNT = 15\n",
    "MAX_TAIL_GAP = 1.30\n",
    "TAIL_TOL = 0.05\n",
    "HIGH_K_BG = 8.0\n",
    "BASE_K_BG = 3.0\n",
    "\n",
    "mask_strict = (\n",
    "    (BG_AN_summary_df[\"Anomaly Count (> Œº+2œÉ)\"] >= MIN_ANOM_COUNT) &\n",
    "    (BG_AN_summary_df[\"tail_gap_R\"] <= MAX_TAIL_GAP) &\n",
    "    (BG_AN_summary_df[\"k_BG\"] >= BASE_K_BG)\n",
    ")\n",
    "\n",
    "mask_soft = (\n",
    "    (BG_AN_summary_df[\"Anomaly Count (> Œº+2œÉ)\"] >= MIN_ANOM_COUNT) &\n",
    "    (BG_AN_summary_df[\"tail_gap_R\"] > MAX_TAIL_GAP) &\n",
    "    (BG_AN_summary_df[\"tail_gap_R\"] <= MAX_TAIL_GAP + TAIL_TOL) &\n",
    "    (BG_AN_summary_df[\"k_BG\"] >= HIGH_K_BG)\n",
    ")\n",
    "\n",
    "mask_core = mask_strict | mask_soft\n",
    "mask_optional = ~mask_core & (BG_AN_summary_df[\"Anomaly Count (> Œº+2œÉ)\"] >= (MIN_ANOM_COUNT - 1))\n",
    "\n",
    "BG_AN_summary_df[\"Group\"] = np.where(\n",
    "    mask_core, \"core\",\n",
    "    np.where(mask_optional, \"optional\", \"excluded\")\n",
    ")\n",
    "\n",
    "# --- Print classification results ---\n",
    "core_elements = BG_AN_summary_df.loc[BG_AN_summary_df[\"Group\"] == \"core\", \"Element\"].tolist()\n",
    "optional_elements = BG_AN_summary_df.loc[BG_AN_summary_df[\"Group\"] == \"optional\", \"Element\"].tolist()\n",
    "excluded_elements = BG_AN_summary_df.loc[BG_AN_summary_df[\"Group\"] == \"excluded\", \"Element\"].tolist()\n",
    "\n",
    "print(\"‚úÖ Core anomaly elements:\", \", \".join(core_elements) if core_elements else \"‚Äî none ‚Äî\")\n",
    "print(\"‚ÑπÔ∏è Optional elements:\", \", \".join(optional_elements) if optional_elements else \"‚Äî none ‚Äî\")\n",
    "print(\"‚ùå Excluded elements:\", \", \".join(excluded_elements) if excluded_elements else \"‚Äî none ‚Äî\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6415f6-1280-4ebe-bba3-a3431834f809",
   "metadata": {},
   "source": [
    "### Block 12: Element Scoring ‚Äî Integrated Indicative Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef68186-5f42-42d9-95b2-26de1ed6d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize metrics for scoring ---\n",
    "TAIL_CAP = 0.30  # cap for tail deviation penalty\n",
    "W_ANOM, W_CONT, W_TAIL = 0.50, 0.30, 0.20  # weights for scoring\n",
    "\n",
    "anom_max = BG_AN_summary_df[\"Anomaly Count (> Œº+2œÉ)\"].replace(0, np.nan).max()\n",
    "cont_max = BG_AN_summary_df[\"k_BG\"].replace(0, np.nan).max()\n",
    "\n",
    "BG_AN_summary_df[\"I_anom\"] = BG_AN_summary_df[\"Anomaly Count (> Œº+2œÉ)\"] / anom_max\n",
    "BG_AN_summary_df[\"I_cont\"] = BG_AN_summary_df[\"k_BG\"] / cont_max\n",
    "\n",
    "# Tail stability: closer to R=1 is better\n",
    "abs_dev = (BG_AN_summary_df[\"tail_gap_R\"] - 1.0).abs()\n",
    "BG_AN_summary_df[\"I_tail\"] = 1.0 - np.minimum(abs_dev, TAIL_CAP) / TAIL_CAP\n",
    "\n",
    "# Final indicative score\n",
    "BG_AN_summary_df[\"S_elem\"] = (\n",
    "    W_ANOM * BG_AN_summary_df[\"I_anom\"] +\n",
    "    W_CONT * BG_AN_summary_df[\"I_cont\"] +\n",
    "    W_TAIL * BG_AN_summary_df[\"I_tail\"]\n",
    ")\n",
    "\n",
    "# Strip '_ppm' suffix for cleaner display\n",
    "def strip_suffix(s: str, suffix=\"_ppm\") -> str:\n",
    "    return s[:-len(suffix)] if isinstance(s, str) and s.endswith(suffix) else s\n",
    "\n",
    "BG_AN_summary_df[\"Element_symbol\"] = BG_AN_summary_df[\"Element\"].apply(strip_suffix)\n",
    "\n",
    "# Sort and export\n",
    "BG_AN_summary_df_sorted = BG_AN_summary_df.sort_values(\"S_elem\", ascending=False)\n",
    "summary_path = OUT_DIR_CSV / \"elements_summary_with_scores.csv\"\n",
    "BG_AN_summary_df_sorted.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"üìÅ Element scoring summary exported: {summary_path.resolve()}\")\n",
    "display(BG_AN_summary_df_sorted[[\"Element\", \"S_elem\", \"Group\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a987c0-6014-4f6b-b997-2bd131551797",
   "metadata": {},
   "source": [
    "### Block 13: Aggregation by Mineralization Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334d109-05f1-42f4-a5c2-bc76f9723ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define mineralization types and associated elements ---\n",
    "types = {\n",
    "    \"Zone 1 (Cu‚ÄìMo‚ÄìW‚ÄìBi)\": [\"Cu\", \"Mo\", \"W\", \"Bi\"],\n",
    "    \"Zone 2 (Pb‚ÄìZn‚ÄìSb‚ÄìMn)\": [\"Pb\", \"Zn\", \"Sb\", \"Mn\"],\n",
    "    \"Zone 3 (Au‚ÄìAg‚ÄìSb‚ÄìSe)\": [\"Au\", \"Ag\", \"Sb\", \"Se\"]\n",
    "} # specify your own complex of elements\n",
    "\n",
    "agg_rows = []\n",
    "for tname, els in types.items():\n",
    "    sub = BG_AN_summary_df[BG_AN_summary_df[\"Element_symbol\"].isin(els)]\n",
    "    if sub.empty:\n",
    "        agg_rows.append({\n",
    "            \"Type\": tname,\n",
    "            \"Elements Included\": 0,\n",
    "            \"Œ£ Anomalies\": 0,\n",
    "            \"Mean Anomaly Intensity\": np.nan,\n",
    "            \"Mean Contrast\": np.nan,\n",
    "            \"Mean Tail Stability\": np.nan,\n",
    "            \"Mean Indicative Score\": np.nan,\n",
    "            \"Total Indicative Score\": 0.0\n",
    "        })\n",
    "        continue\n",
    "    agg_rows.append({\n",
    "        \"Type\": tname,\n",
    "        \"Elements Included\": len(sub),\n",
    "        \"Œ£ Anomalies\": int(sub[\"Anomaly Count (> Œº+2œÉ)\"].sum()),\n",
    "        \"Mean Anomaly Intensity\": float(sub[\"I_anom\"].mean()),\n",
    "        \"Mean Contrast\": float(sub[\"I_cont\"].mean()),\n",
    "        \"Mean Tail Stability\": float(sub[\"I_tail\"].mean()),\n",
    "        \"Mean Indicative Score\": float(sub[\"S_elem\"].mean()),\n",
    "        \"Total Indicative Score\": float(sub[\"S_elem\"].sum())\n",
    "    })\n",
    "\n",
    "df_types = pd.DataFrame(agg_rows)\n",
    "df_types_sum = df_types.sort_values(\"Total Indicative Score\", ascending=False)\n",
    "df_types_avg = df_types.sort_values(\"Mean Indicative Score\", ascending=False)\n",
    "\n",
    "# --- Export aggregated tables ---\n",
    "df_types_sum.to_csv(OUT_DIR_CSV / \"types_scores_sorted_by_sum.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_types_avg.to_csv(OUT_DIR_CSV / \"types_scores_sorted_by_mean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nüèÅ Mineralization types (sorted by TOTAL score):\")\n",
    "print(df_types_sum.to_string(index=False))\n",
    "\n",
    "print(\"\\nüèÅ Mineralization types (sorted by MEAN score):\")\n",
    "print(df_types_avg.to_string(index=False))\n",
    "\n",
    "\n",
    "# --- Barplot: total indicative score by mineralization type ---\n",
    "plt.figure(figsize=(9, 4.5))\n",
    "sns.barplot(data=df_types_sum, x=\"Total Indicative Score\", y=\"Type\", color=\"#4C9F70\")\n",
    "plt.title(\"Total Contribution by Mineralization Type (Sum of Scores)\")\n",
    "plt.xlabel(\"Total Indicative Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "sum_plot_path = OUT_DIR_PNG / \"types_sum_score.png\"\n",
    "plt.savefig(sum_plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# --- Barplot: mean indicative score by mineralization type ---\n",
    "plt.figure(figsize=(9, 4.5))\n",
    "sns.barplot(data=df_types_avg, x=\"Mean Indicative Score\", y=\"Type\", color=\"#3C7DD9\")\n",
    "plt.title(\"Signal Intensity by Mineralization Type (Mean Score)\")\n",
    "plt.xlabel(\"Mean Indicative Score\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "mean_plot_path = OUT_DIR_PNG / \"types_mean_score.png\"\n",
    "plt.savefig(mean_plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved plots:\")\n",
    "print(f\" ‚Ä¢ Total score ‚Üí {sum_plot_path.resolve()}\")\n",
    "print(f\" ‚Ä¢ Mean score ‚Üí {mean_plot_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f513ac-a44a-4800-864c-05bb7cdd3679",
   "metadata": {},
   "source": [
    "### Block 15: Final Element Selection and Level Rounding Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed96b78-ff4b-462b-a935-0027d998fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final selection of informative elements for mapping ---\n",
    "selected_elements = [\n",
    "    'Cu_ppm', 'Au_ppb', 'Ag_ppm', 'Mo_ppm', 'W_ppm', 'Bi_ppm',\n",
    "    'Sb_ppm', 'Sn_ppm', 'Pb_ppm', 'Zn_ppm', 'Mn_ppm', 'Se_ppm'\n",
    "]\n",
    "\n",
    "# --- Rounding steps for map readability ---\n",
    "ELEMENTS_ROUND_STEP = {\n",
    "    'Cu_ppm': 50.0,\n",
    "    'Au_ppb': 10.0,\n",
    "    'Ag_ppm': 0.5,\n",
    "    'Mo_ppm': 1.0,\n",
    "    'W_ppm': 5.0,\n",
    "    'Bi_ppm': 1.0,\n",
    "    'Sb_ppm': 1.0,\n",
    "    'Sn_ppm': 1.0,\n",
    "    'Pb_ppm': 100.0,\n",
    "    'Zn_ppm': 100.0,\n",
    "    'Mn_ppm': 500.0,\n",
    "    'Se_ppm': 2.0\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Selected informative elements and rounding steps configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04b28e-3363-42ce-a0ee-13cf1222b363",
   "metadata": {},
   "source": [
    "### Block 16: Mapping Configuration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967542dc-1f79-479f-9f30-286f7e7de48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for mapping pipeline ---\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from decimal import Decimal\n",
    "\n",
    "# Smoothing and interpolation\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Geometry and GIS\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import (\n",
    "    Polygon, MultiPolygon, Point, LineString, MultiPoint, LinearRing\n",
    ")\n",
    "from shapely.ops import unary_union\n",
    "from shapely.prepared import prep as prep_geom\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry.base import JOIN_STYLE\n",
    "\n",
    "# Contour extraction from masks\n",
    "from skimage import measure\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Coordinate system for export\n",
    "    crs_epsg: str = \"EPSG:00000\"  # WGS 84 / INSERT HERE YOUR OWN COORDINATE SYSTEM\n",
    "\n",
    "    # Sigma level configuration\n",
    "    sigma_step: float = 0.25\n",
    "    winsorize_enabled: bool = True\n",
    "    winsor_k_sigma: float = 3.0\n",
    "\n",
    "    # Edge anomaly rounding and view padding\n",
    "    buffer_m: float = 200.0\n",
    "    view_padding_m: float = 400.0\n",
    "    ring_spacing_m: float = 50.0\n",
    "\n",
    "    # Interpolation and smoothing\n",
    "    grid_n: int = 500\n",
    "    smooth_m: float = 60.0\n",
    "\n",
    "    # Polygon smoothing (in/out buffer)\n",
    "    smooth_radius_m: float = 60.0\n",
    "\n",
    "    # Chaikin smoothing parameters\n",
    "    b_maxseg_m: float = 15.0\n",
    "    b_iter: int = 2\n",
    "\n",
    "    # Minimum polygon area (m¬≤)\n",
    "    min_area_m2: float = 7000.0\n",
    "\n",
    "    # SHP attribute encoding\n",
    "    encoding: str = \"UTF-8\"\n",
    "\n",
    "CFG = Config()\n",
    "print(\"‚úÖ Mapping pipeline configuration initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471d3eb-14f9-4dbe-a9f7-c8618e6e951d",
   "metadata": {},
   "source": [
    "###  Block 17: Mapping Functions ‚Äî Interpolation, Smoothing, Contour Extraction, Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6b739-9267-49fb-8019-092e5803d574",
   "metadata": {},
   "source": [
    "#### Block 17.1: Utility Functions for Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b407c4-5d42-42bf-8e48-9fb88f1c2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Determine decimal places from rounding step ---\n",
    "def decimals_from_step(step: float) -> int:\n",
    "    \"\"\"\n",
    "    Determines number of decimal places needed for a given rounding step.\n",
    "    \"\"\"\n",
    "    d = Decimal(str(step)).as_tuple().exponent\n",
    "    return max(0, -d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec21dcd-6480-4e63-9f99-d4ed0cf2fbb4",
   "metadata": {},
   "source": [
    "#### Block 17.2: Winsorization in Log-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c648f60-03f3-40a5-b006-2e714f14b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_log(z: np.ndarray, k: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Soft clipping of tails: limits log(z) to [Œº‚àíkœÉ, Œº+kœÉ], returns values in linear scale.\n",
    "    \"\"\"\n",
    "    zp = np.asarray(z, float)\n",
    "    msk = zp > 0\n",
    "    if msk.sum() < 2:\n",
    "        return zp\n",
    "    logz = np.log(zp[msk])\n",
    "    mu, sd = float(np.mean(logz)), float(np.std(logz, ddof=1))\n",
    "    lo, hi = mu - k * sd, mu + k * sd\n",
    "    logz = np.clip(logz, lo, hi)\n",
    "    zp[msk] = np.exp(logz)\n",
    "    return zp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e83ad-d730-4ddb-a04c-3396a37c011a",
   "metadata": {},
   "source": [
    "#### Block 17.3: Merge Nearby Sample Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba86ab0-ce68-4b6c-bc14-594747e6b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge nearby sample points ---\n",
    "def merge_close_points(x: np.ndarray, y: np.ndarray, z: np.ndarray, radius_m: float):\n",
    "    \"\"\"\n",
    "    Merges spatially close samples within 'radius_m' using a union-find over pairwise neighbors.\n",
    "    Returns averaged x, y and median z per cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : arrays of coordinates (meters, projected CRS)\n",
    "    z : array of values (positive)\n",
    "    radius_m : clustering radius in meters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xv2, yv2, zv2 : numpy arrays of merged coordinates and values\n",
    "    \"\"\"\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    coords = np.c_[x, y]\n",
    "    if len(coords) == 0:\n",
    "        return x, y, z\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    pairs = tree.query_pairs(r=radius_m)\n",
    "\n",
    "    parent = list(range(len(coords)))\n",
    "\n",
    "    def find(a):\n",
    "        while parent[a] != a:\n",
    "            parent[a] = parent[parent[a]]\n",
    "            a = parent[a]\n",
    "        return a\n",
    "\n",
    "    def union(a, b):\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            parent[rb] = ra\n",
    "\n",
    "    for i, j in pairs:\n",
    "        union(i, j)\n",
    "\n",
    "    clusters = {}\n",
    "    for i in range(len(coords)):\n",
    "        r = find(i)\n",
    "        clusters.setdefault(r, []).append(i)\n",
    "\n",
    "    xv2, yv2, zv2 = [], [], []\n",
    "    for ids in clusters.values():\n",
    "        xv2.append(float(np.mean(x[ids])))\n",
    "        yv2.append(float(np.mean(y[ids])))\n",
    "        zv2.append(float(np.median(z[ids])))\n",
    "\n",
    "    return np.array(xv2), np.array(yv2), np.array(zv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d15c60-a9d1-4527-b97c-e888735612f8",
   "metadata": {},
   "source": [
    "#### Block 17.4: Sigma Level Generation and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225fc21-055a-4497-819e-28a8b3cb16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sigma label for a contour level ---\n",
    "def sigma_label_for_level(L: float, mu: float, sigma: float, step: float) -> str:\n",
    "    k_val = (np.log(L) - mu) / sigma\n",
    "    k_val = float(np.round(k_val / step) * step)\n",
    "    if abs(k_val - int(k_val)) < 1e-9:\n",
    "        return f\"{int(k_val)}œÉ\"\n",
    "    k_str = f\"{k_val:.2f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return f\"{k_str}œÉ\"\n",
    "\n",
    "\n",
    "# --- Generate sigma-based contour levels in linear space ---\n",
    "def make_sigma_levels(mu: float, sigma: float, zmax: float,\n",
    "                      sigma_step: float, round_step: float) -> list[float]:\n",
    "    k = sigma_step\n",
    "    raw = []\n",
    "    while True:\n",
    "        L = float(np.exp(mu + k * sigma))\n",
    "        if L > zmax: break\n",
    "        raw.append(L); k += sigma_step\n",
    "    if not raw: return []\n",
    "    levels = np.unique(np.round(np.array(raw) / round_step) * round_step)\n",
    "    return [float(v) for v in levels if 0 < v <= zmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379780c-d1a0-464c-8f6b-3a3283f6dd28",
   "metadata": {},
   "source": [
    "#### Block 17.5: Extract polygons from a boolean mask (with holes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca42600-d343-4daf-85d7-931f3c4907a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_from_mask(mask_bool: np.ndarray, GX: np.ndarray, GY: np.ndarray):\n",
    "    \"\"\"\n",
    "    Converts a boolean exceedance mask (ny x nx) into Shapely polygons using image contours.\n",
    "    Ensures correct assignment of interior holes to outer rings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_bool : 2D boolean array\n",
    "    GX, GY    : 2D arrays of the same shape with grid x/y in map units\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of shapely.Polygon (holes preserved) and/or MultiPolygon pieces\n",
    "    \"\"\"\n",
    "    assert mask_bool.shape == GX.shape == GY.shape, \"Mask and grid shapes must match\"\n",
    "    ny, nx = mask_bool.shape\n",
    "    x = GX[0, :]\n",
    "    y = GY[:, 0]\n",
    "\n",
    "    labels, nlabels = measure.label(mask_bool.astype(np.uint8), connectivity=2, return_num=True)\n",
    "    polys_out = []\n",
    "\n",
    "    def _img_to_world(contour_rc):\n",
    "        rows, cols = contour_rc[:, 0], contour_rc[:, 1]\n",
    "        xs = np.interp(cols, np.arange(nx), x)\n",
    "        ys = np.interp(rows, np.arange(ny), y)\n",
    "        return np.c_[xs, ys]\n",
    "\n",
    "    def _signed_area_img(contour_rc):\n",
    "        pts = np.c_[contour_rc[:, 1], contour_rc[:, 0]]\n",
    "        return 0.5 * np.sum(pts[:-1, 0] * pts[1:, 1] - pts[1:, 0] * pts[:-1, 1])\n",
    "\n",
    "    for lbl in range(1, nlabels + 1):\n",
    "        roi = (labels == lbl)\n",
    "        if roi.sum() < 4:\n",
    "            continue\n",
    "\n",
    "        contours = measure.find_contours(roi.astype(np.uint8), 0.5)\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        outers_img, holes_img = [], []\n",
    "        for c in contours:\n",
    "            if len(c) < 3:\n",
    "                continue\n",
    "            A = _signed_area_img(c)\n",
    "            (outers_img if A > 0 else holes_img).append(c)\n",
    "\n",
    "        if not outers_img:\n",
    "            # Fallback: treat the largest area as the outer\n",
    "            areas = [abs(_signed_area_img(c)) for c in contours]\n",
    "            outer_idx = int(np.argmax(areas))\n",
    "            outers_img = [contours[outer_idx]]\n",
    "            holes_img = [c for i, c in enumerate(contours) if i != outer_idx]\n",
    "\n",
    "        outers_world = [_img_to_world(c) for c in outers_img]\n",
    "        holes_world  = [_img_to_world(c) for c in holes_img]\n",
    "\n",
    "        outer_geoms = [Polygon(outer) for outer in outers_world]\n",
    "        hole_geoms  = [Polygon(h) for h in holes_world]\n",
    "\n",
    "        if outer_geoms:\n",
    "            outer_tree = STRtree(outer_geoms)\n",
    "            holes_by_outer = {id(g): [] for g in outer_geoms}\n",
    "\n",
    "            # Assign each hole to the containing outer polygon\n",
    "            for h in hole_geoms:\n",
    "                cand = outer_tree.query(h.centroid)\n",
    "                # 'cand' can be indices or geometries depending on env; normalize to list of geoms\n",
    "                if isinstance(cand, (list, tuple)):\n",
    "                    candidates = cand\n",
    "                else:\n",
    "                    try:\n",
    "                        candidates = [outer_geoms[int(i)] for i in np.atleast_1d(cand)]\n",
    "                    except Exception:\n",
    "                        candidates = outer_geoms\n",
    "\n",
    "                for o in candidates:\n",
    "                    if o.contains(h):\n",
    "                        holes_by_outer[id(o)].append(h)\n",
    "                        break\n",
    "            for o in outer_geoms:\n",
    "                holes_coords = [\n",
    "                    h.exterior.coords[:] for h in holes_by_outer[id(o)]\n",
    "                    if h.exterior and len(h.exterior.coords) >= 4\n",
    "                ]\n",
    "                poly = Polygon(o.exterior.coords[:], holes=holes_coords).buffer(0)\n",
    "                if not poly.is_empty and poly.is_valid:\n",
    "                    polys_out.append(poly)\n",
    "\n",
    "    return polys_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c51119-f5c3-4006-ac92-1a8cc84c1aa1",
   "metadata": {},
   "source": [
    "#### Block 17.6: Geometric smoothing (Chaikin corner cutting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63dfba-bc8c-4903-8688-ddbe7288c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Densify a LinearRing to limit max segment length (meters) ---\n",
    "def _densify_ring(line: LinearRing, maxseg_m: float) -> LineString:\n",
    "    L = float(line.length)\n",
    "    n = max(8, int(np.ceil(L / maxseg_m)))\n",
    "    pts = [line.interpolate(i / n, normalized=True) for i in range(n)]\n",
    "    pts.append(pts[0])\n",
    "    return LineString(pts)\n",
    "\n",
    "# --- One iteration of Chaikin smoothing (closed ring supported) ---\n",
    "def _chaikin(coords: np.ndarray, keep_closed: bool = True) -> np.ndarray:\n",
    "    P = np.asarray(coords, float)\n",
    "    closed = keep_closed and np.allclose(P[0], P[-1])\n",
    "    if closed:\n",
    "        P = P[:-1]\n",
    "    Q = []\n",
    "    for i in range(len(P)):\n",
    "        p0 = P[i]\n",
    "        p1 = P[(i + 1) % len(P)]\n",
    "        Q.append(0.75 * p0 + 0.25 * p1)\n",
    "        Q.append(0.25 * p0 + 0.75 * p1)\n",
    "    Q = np.vstack(Q)\n",
    "    if keep_closed:\n",
    "        Q = np.vstack([Q, Q[0]])\n",
    "    return Q\n",
    "\n",
    "# --- Chaikin smoothing for a polygon (exterior + interiors) ---\n",
    "def smooth_polygon_chaikin(poly: Polygon, maxseg_m: float = 15.0, n_iter: int = 2) -> Polygon:\n",
    "    # Exterior\n",
    "    ext = _densify_ring(LinearRing(poly.exterior.coords), maxseg_m)\n",
    "    ext_coords = np.array(ext.coords)\n",
    "    for _ in range(max(1, int(n_iter))):\n",
    "        ext_coords = _chaikin(ext_coords, keep_closed=True)\n",
    "\n",
    "    # Interiors (holes)\n",
    "    holes_coords = []\n",
    "    for hole in poly.interiors:\n",
    "        hh = _densify_ring(LinearRing(hole.coords), maxseg_m)\n",
    "        hc = np.array(hh.coords)\n",
    "        for _ in range(max(1, int(n_iter - 1))):\n",
    "            hc = _chaikin(hc, keep_closed=True)\n",
    "        if len(hc) >= 4:\n",
    "            holes_coords.append(hc)\n",
    "\n",
    "    return Polygon(ext_coords, holes=holes_coords).buffer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d3f14-a0c4-4256-b58f-5e8ff162d1e2",
   "metadata": {},
   "source": [
    "#### Block 17.7: Element map plotting (filled contours + lines, points, buffer, scalebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d933c7c-a693-4b0a-b05a-581aa921f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot element map (filled contours, isolines, samples, buffer, scalebar) ---\n",
    "def plot_element_map(\n",
    "    GX, GY, GZ_final, levels, hull_buf, xv, yv,\n",
    "    title: str, element_name: str,\n",
    "    out_png: Optional[Path] = None,\n",
    "    buffer_m: float = 0.0, smooth_m: float = 0.0,\n",
    "    cmap: str = \"turbo\", value_decimals: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws filled contours + isolines for 'GZ_final' clipped by hull buffer; overlays sample points,\n",
    "    buffer outline, simple north arrow and a metric scalebar.\n",
    "    \"\"\"\n",
    "    # Safe colormap\n",
    "    try:\n",
    "        plt.get_cmap(cmap)\n",
    "    except Exception:\n",
    "        cmap = \"viridis\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 9.5))\n",
    "\n",
    "    mgrid = np.ma.masked_invalid(GZ_final)\n",
    "    cf = ax.contourf(GX, GY, mgrid, levels=levels, cmap=cmap, alpha=0.72, antialiased=True)\n",
    "    cs = ax.contour(GX, GY, mgrid, levels=levels, colors=\"k\", linewidths=0.45)\n",
    "\n",
    "    # Optional labels on a subset of isolines\n",
    "    if len(levels) >= 1:\n",
    "        try:\n",
    "            step = max(1, len(levels) // 8)  # avoid clutter\n",
    "            ax.clabel(cs, levels=levels[::step], fmt=lambda v: f\"{int(v)}\", inline=True, fontsize=8)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Samples and buffer outline\n",
    "    ax.scatter(xv, yv, c=\"black\", s=6, edgecolors=\"none\", alpha=0.55, zorder=3, label=\"Samples\")\n",
    "    rx, ry = hull_buf.exterior.xy\n",
    "    ax.plot(rx, ry, color=\"white\", lw=1.4, ls=\"--\", alpha=0.9, label=f\"Convex hull +{int(buffer_m)} m\")\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"East (m)\"); ax.set_ylabel(\"North (m)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\", frameon=True)\n",
    "    ax.set_xticklabels([]); ax.set_yticklabels([])\n",
    "\n",
    "    # Simple north arrow\n",
    "    ax.annotate('N', xy=(0.95, 0.95), xytext=(0.95, 0.85),\n",
    "                xycoords='axes fraction', textcoords='axes fraction',\n",
    "                arrowprops=dict(facecolor='black', width=2, headwidth=10),\n",
    "                ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(cf, ax=ax)\n",
    "    cbar.set_label(f\"{element_name}: rounded œÉ-levels\")\n",
    "\n",
    "    # Metric scalebar (3 segments of equal length)\n",
    "    scale_origin_x = xv.min() + (xv.max() - xv.min()) / 2 - 300\n",
    "    scale_origin_y = yv.min() - 50\n",
    "    segment_length = 400  # meters per segment\n",
    "    segment_height = 50\n",
    "\n",
    "    for i in range(3):\n",
    "        color = 'black' if i % 2 == 0 else 'white'\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (scale_origin_x + i * segment_length, scale_origin_y),\n",
    "                segment_length, segment_height,\n",
    "                facecolor=color, edgecolor='black', zorder=4\n",
    "            )\n",
    "        )\n",
    "    total_length = segment_length * 3\n",
    "    ax.add_patch(\n",
    "        Rectangle((scale_origin_x, scale_origin_y), total_length, segment_height,\n",
    "                  fill=False, edgecolor='black', linewidth=1.2, zorder=5)\n",
    "    )\n",
    "    for i in range(4):\n",
    "        ax.text(scale_origin_x + i * segment_length, scale_origin_y - 15,\n",
    "                f\"{i * segment_length}\", ha='center', va='top', fontsize=10)\n",
    "    ax.text(scale_origin_x + total_length + 20, scale_origin_y + segment_height / 2,\n",
    "            \"m\", ha='left', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if out_png is not None:\n",
    "        fig.savefig(out_png, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c48181-64f5-487e-9414-9b9939a05fc6",
   "metadata": {},
   "source": [
    "#### Block 17.8: Export polygons (to SHP) and the PNG map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3e0d2-5e15-4849-95a7-de9ca2be5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export SHP/PNG for a set of polygons at multiple levels ---\n",
    "def export_records(\n",
    "    records: list[dict],\n",
    "    GX, GY, GZ_final, levels, hull_buf, xv, yv,\n",
    "    element_name: str,\n",
    "    cfg: Config,\n",
    "    out_png_path: Optional[Path] = None,\n",
    "    title_suffix: str = \"\",\n",
    "    value_decimals: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a GeoDataFrame from 'records' and writes:\n",
    "      - ESRI Shapefile with attributes (Element, Unit, Value, SigmaK, Area_m2/km2)\n",
    "      - PNG map via 'plot_element_map' if 'out_png_path' is provided.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        print(f\"‚ö†Ô∏è No polygons created for {element_name}. Skipping export.\")\n",
    "        return\n",
    "\n",
    "    crs_str = str(cfg.crs_epsg) if cfg.crs_epsg else None\n",
    "    gdf = gpd.GeoDataFrame(records, geometry=\"geometry\", crs=crs_str)\n",
    "\n",
    "    # Normalize attribute dtypes\n",
    "    gdf[\"Element\"] = gdf[\"Element\"].astype(str)\n",
    "    gdf[\"Unit\"] = gdf[\"Unit\"].astype(str)\n",
    "    gdf[\"Value\"] = gdf[\"Value\"].astype(\"float64\")\n",
    "    gdf[\"SigmaK\"] = gdf[\"SigmaK\"].astype(str)\n",
    "\n",
    "    # Explode multipolygons\n",
    "    gdf = gdf.explode(index_parts=False, ignore_index=True)\n",
    "\n",
    "    # Paths\n",
    "    shp_path = OUT_DIR_SHP / f\"{element_name}_contours_B_chaikin.shp\"\n",
    "    png_path = out_png_path or (OUT_DIR_PNG / f\"{element_name}_sigma_rounded_B_chaikin.png\")\n",
    "\n",
    "    # Export SHP\n",
    "    gdf.to_file(shp_path, driver=\"ESRI Shapefile\", encoding=cfg.encoding)\n",
    "\n",
    "    # Compose title and export PNG\n",
    "    title = (\n",
    "        f\"{element_name} : rounded anomaly levels\\n\"\n",
    "        f\"Interpolation +{int(cfg.buffer_m)} m; BG = geom. mean; smoothing ‚âà{int(cfg.smooth_m)} m\"\n",
    "    )\n",
    "    plot_element_map(\n",
    "        GX, GY, GZ_final, levels, hull_buf, xv, yv,\n",
    "        title=title, element_name=element_name,\n",
    "        out_png=png_path,\n",
    "        buffer_m=cfg.buffer_m, smooth_m=cfg.smooth_m, cmap=\"turbo\",\n",
    "        value_decimals=value_decimals\n",
    "    )\n",
    "    print(f\"‚úÖ SHP saved:  {shp_path.resolve()}\")\n",
    "    print(f\"‚úÖ PNG saved:  {png_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5c6ca-69a6-4289-8019-76da1192304c",
   "metadata": {},
   "source": [
    "#### Block 17.9: End‚Äëto‚Äëend processing for one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef459def-9ccd-4080-ae83-98f108970414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full mapping pipeline for a single element ---\n",
    "def process_element(\n",
    "    df: pd.DataFrame,\n",
    "    element_name: str,\n",
    "    round_step_value: float,\n",
    "    cfg: Config = CFG,\n",
    "    merge_nearby_enabled: bool = True,\n",
    "    merge_radius_m: float = 25.0\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end workflow for one element:\n",
    "      1) Filter positive values, optionally merge close samples and winsorize tails (log-space).\n",
    "      2) Estimate mu, sigma in log-space; generate sigma levels and round to 'round_step_value'.\n",
    "      3) Interpolate to regular grid with soft boundary (background = geometric mean).\n",
    "      4) Gaussian smoothing (approx. 'smooth_m' meters).\n",
    "      5) Extract polygons per level, apply buffer-in/out rounding and Chaikin smoothing.\n",
    "      6) Export SHP and a single PNG map.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires 'x', 'y' coordinate columns in meters (projected CRS).\n",
    "    - 'round_step_value' should match human-readable step for the element (e.g., 50 ppm).\n",
    "    \"\"\"\n",
    "    if element_name not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Column '{element_name}' not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Coordinates and values\n",
    "    x = pd.to_numeric(df[\"x\"], errors=\"coerce\").to_numpy()\n",
    "    y = pd.to_numeric(df[\"y\"], errors=\"coerce\").to_numpy()\n",
    "    z = pd.to_numeric(df[element_name], errors=\"coerce\").to_numpy()\n",
    "    mask = np.isfinite(x) & np.isfinite(y) & np.isfinite(z) & (z > 0)\n",
    "    xv, yv, zv = x[mask], y[mask], z[mask]\n",
    "\n",
    "    if len(zv) < 10:\n",
    "        print(f\"‚ö†Ô∏è Not enough points for '{element_name}' ({len(zv)} < 10). Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Optional pre-processing\n",
    "    if merge_nearby_enabled and merge_radius_m > 0:\n",
    "        xv, yv, zv = merge_close_points(xv, yv, zv, merge_radius_m)\n",
    "    if cfg.winsorize_enabled:\n",
    "        zv = winsorize_log(zv, cfg.winsor_k_sigma)\n",
    "\n",
    "    # Log-space stats\n",
    "    log_vals = np.log(zv)\n",
    "    mu = float(np.mean(log_vals))\n",
    "    sigma = float(np.std(log_vals, ddof=1))\n",
    "    z_max = float(np.max(zv))\n",
    "    geo_mean = float(np.exp(mu))\n",
    "\n",
    "    # Sigma levels (rounded)\n",
    "    levels = make_sigma_levels(mu, sigma, z_max, cfg.sigma_step, round_step_value)\n",
    "    if not levels:\n",
    "        print(f\"‚ö†Ô∏è No œÉ-levels for '{element_name}'. Skipping.\")\n",
    "        return\n",
    "    decimals = decimals_from_step(round_step_value)\n",
    "\n",
    "    # Convex hull buffer and background ring\n",
    "    hull = MultiPoint(list(zip(xv, yv))).convex_hull\n",
    "    hull_buf = hull.buffer(cfg.buffer_m, resolution=64, join_style=JOIN_STYLE.round)\n",
    "    ring = LineString(hull_buf.exterior.coords)\n",
    "    perim = ring.length\n",
    "    n_pts = max(8, int(math.ceil(perim / cfg.ring_spacing_m)))\n",
    "    ring_x, ring_y = [], []\n",
    "    for i in range(n_pts):\n",
    "        p = ring.interpolate((i / n_pts) * perim)\n",
    "        ring_x.append(p.x); ring_y.append(p.y)\n",
    "    ring_val = geo_mean\n",
    "    ring_z = np.full(len(ring_x), ring_val, dtype=float)\n",
    "\n",
    "    # Merge real and background samples\n",
    "    x_ext = np.concatenate([xv, np.array(ring_x)])\n",
    "    y_ext = np.concatenate([yv, np.array(ring_y)])\n",
    "    z_ext = np.concatenate([zv, ring_z])\n",
    "\n",
    "    # Grid and interpolation\n",
    "    xmin, xmax = xv.min() - cfg.view_padding_m, xv.max() + cfg.view_padding_m\n",
    "    ymin, ymax = yv.min() - cfg.view_padding_m, yv.max() + cfg.view_padding_m\n",
    "    gx = np.linspace(xmin, xmax, cfg.grid_n)\n",
    "    gy = np.linspace(ymin, ymax, cfg.grid_n)\n",
    "    GX, GY = np.meshgrid(gx, gy)\n",
    "\n",
    "    try:\n",
    "        GZ = griddata((x_ext, y_ext), z_ext, (GX, GY), method=\"cubic\")\n",
    "    except Exception:\n",
    "        GZ = griddata((x_ext, y_ext), z_ext, (GX, GY), method=\"linear\")\n",
    "    GZ_near = griddata((x_ext, y_ext), z_ext, (GX, GY), method=\"nearest\")\n",
    "    GZ = np.where(np.isfinite(GZ), GZ, GZ_near)\n",
    "\n",
    "    # Inside-buffer mask (soft boundary)\n",
    "    try:\n",
    "        prep_buf = prep_geom(hull_buf)\n",
    "    except Exception:\n",
    "        prep_buf = None\n",
    "    inside_mask = np.zeros(GX.shape, dtype=bool)\n",
    "    for i in range(GX.shape[0]):\n",
    "        row_pts = [Point(float(GX[i, j]), float(GY[i, j])) for j in range(GX.shape[1])]\n",
    "        if prep_buf is not None:\n",
    "            try:\n",
    "                inside_mask[i, :] = [prep_buf.covers(pt) for pt in row_pts]\n",
    "            except Exception:\n",
    "                inside_mask[i, :] = [hull_buf.covers(pt) for pt in row_pts]\n",
    "        else:\n",
    "            inside_mask[i, :] = [hull_buf.covers(pt) for pt in row_pts]\n",
    "\n",
    "    # Gaussian smoothing (convert meters to grid std-dev)\n",
    "    dx = (xmax - xmin) / (cfg.grid_n - 1)\n",
    "    dy = (ymax - ymin) / (cfg.grid_n - 1)\n",
    "    sigx = max(0.1, cfg.smooth_m / dx)\n",
    "    sigy = max(0.1, cfg.smooth_m / dy)\n",
    "\n",
    "    WZ = GZ.copy()\n",
    "    WZ[~inside_mask] = ring_val  # soft background\n",
    "    WZ_smooth = gaussian_filter(WZ, sigma=(sigy, sigx), mode=\"nearest\")\n",
    "    GZ_final = np.where(inside_mask, WZ_smooth, np.nan)\n",
    "\n",
    "    # Polygonize per level, geometric rounding + Chaikin smoothing\n",
    "    unit = parse_unit(element_name) if \"parse_unit\" in globals() else \"\"\n",
    "    records = []\n",
    "    for L in levels:\n",
    "        base_mask = np.isfinite(GZ_final) & (GZ_final >= L)\n",
    "        polys_L = polygons_from_mask(base_mask, GX, GY)\n",
    "        if not polys_L:\n",
    "            continue\n",
    "        sigma_lbl = sigma_label_for_level(L, mu, sigma, step=cfg.sigma_step)\n",
    "\n",
    "        for poly in polys_L:\n",
    "            # Geometric rounding of corners via in/out buffer\n",
    "            if cfg.smooth_radius_m and cfg.smooth_radius_m > 0:\n",
    "                poly = (\n",
    "                    poly.buffer(cfg.smooth_radius_m, resolution=64, join_style=JOIN_STYLE.round)\n",
    "                        .buffer(-cfg.smooth_radius_m, resolution=64, join_style=JOIN_STYLE.round)\n",
    "                )\n",
    "\n",
    "            # Chaikin smoothing + validity fix\n",
    "            poly = smooth_polygon_chaikin(poly, maxseg_m=cfg.b_maxseg_m, n_iter=cfg.b_iter).buffer(0)\n",
    "\n",
    "            # Keep only valid polygons above the area threshold\n",
    "            geoms = [poly] if poly.geom_type == \"Polygon\" else list(poly.geoms)\n",
    "            for p in geoms:\n",
    "                p = p.buffer(0)\n",
    "                if p.is_empty or not p.is_valid:\n",
    "                    continue\n",
    "                if cfg.min_area_m2 and float(p.area) < float(cfg.min_area_m2):\n",
    "                    continue\n",
    "\n",
    "                val_rounded = round(float(L), decimals)\n",
    "                records.append({\n",
    "                    \"Element\": str(element_name),\n",
    "                    \"Unit\": unit,\n",
    "                    \"Value\": float(val_rounded),\n",
    "                    \"SigmaK\": str(sigma_lbl),\n",
    "                    \"Area_m2\": float(p.area),\n",
    "                    \"Area_km2\": float(p.area) / 1e6,\n",
    "                    \"geometry\": p,\n",
    "                })\n",
    "\n",
    "    # Export SHP + PNG (single map per element)\n",
    "    export_records(\n",
    "        records,\n",
    "        GX, GY, GZ_final, levels, hull_buf, xv, yv,\n",
    "        element_name=element_name,\n",
    "        cfg=cfg,\n",
    "        out_png_path=None,\n",
    "        title_suffix=\"\",\n",
    "        value_decimals=decimals,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205789f-6e7e-4155-8325-51a0890d6971",
   "metadata": {},
   "source": [
    "#### Block 17.A: Batch run over selected elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6273e0-04ef-48d7-95f4-a719ac42abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch driver ---\n",
    "assert 'x' in df.columns and 'y' in df.columns, \"DataFrame must contain 'x' and 'y' columns.\"\n",
    "\n",
    "for el in selected_elements:\n",
    "    step_val = ELEMENTS_ROUND_STEP.get(el, None)\n",
    "    if step_val is None:\n",
    "        print(f\"‚ö†Ô∏è No rounding step configured for '{el}'. Skipping.\")\n",
    "        continue\n",
    "    try:\n",
    "        print(f\"\\n=== Processing: {el} (round step {step_val}) ===\")\n",
    "        process_element(df, el, float(step_val), cfg=CFG, merge_nearby_enabled=True, merge_radius_m=25.0)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while processing '{el}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080233b-3d22-49e1-b758-aa0dfa85aace",
   "metadata": {},
   "source": [
    "# Analysis and Export Completed ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
